<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "http://jats.nlm.nih.gov/archiving/1.0/JATS-archivearticle1.dtd">
<article article-type="research-article"><!--article-type="research-article" was added, it does not have to be a research article but it must exist-->
<front><journal-meta><journal-title-group><journal-title>Super Awesome Journal</journal-title></journal-title-group><issn>12345-6778X</issn></journal-meta><article-meta><article-id pub-id-type="doi">10.1111/182</article-id><article-id pub-id-type="pmid">123456789</article-id>
<article-id pub-id-type="pmc">unknown</article-id><!--added line, PMC id is mandatory for PMC papers, it will be used to generate the name of the output files-->
<title-group><article-title>Notes on Operations: Automated Metadata Harvesting:  Low-Barrier MARC Record  Generation from OAI-PMH  Repository Stores Using MarcEdit</article-title></title-group><contrib-group><contrib contrib-type="author"><string-name>Terry Reese</string-name></contrib><contrib contrib-type="author"><string-name>Bill Clinton</string-name></contrib></contrib-group><pub-date pub-type="pub"><year>2012</year></pub-date><volume>12</volume><issue>6</issue><isbn>12345-6789X</isbn><fpage>234</fpage><lpage>238</lpage><abstract><p>For libraries, the burgeoning corpus of born-digital data is becoming both a blessing and a curse. For patrons, these online resources represent the potential for extended access to materials, but for a library&#x2019;s technical services department they represent an ongoing challenge, forcing staff to look for ways to capture and make use of available metadata. This challenge is exacerbated for libraries that provide access to their own digital collections. While digital repository software like DSpace, Fedora, and CONTENTdm expose bibliographic metadata through the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH), few organizations have a simplified method for harvesting and generating MachineReadable Cataloging (MARC) records from these metadata stores. Fortunately, a number of tools have been developed that can facilitate the harvesting and generation of MARC data from these OAI-PMH metadata repositories. This paper will examine resources that enhance technical services staff&#x2019;s ability to use existing metadata, with specific focus on one of these current generation tools, MarcEdit, which was developed by the author and provides a one-click harvesting process for generating MARC metadata from a variety of metadata formats</p></abstract><kwd-group><kwd>marc</kwd><kwd>pdf</kwd><kwd>xml</kwd><kwd>record</kwd></kwd-group></article-meta></front><body><sec><title>Introduction</title><p>On December 11, 2007, Perry Willett, head of the Digital Library Production Service at the University of Michigan (UM) Library, posted a message to the XML4Lib electronic discussion list indicating that metadata for the public domain materials made available through the UM Library Google Books project were now available for Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) download.1 The OAI-PMH protocol was primarily developed as a low-barrier method for interoperability between metadata repositories. Using the protocol, structured bibliographic metadata can be shared between repositories and other metadata harvesters. The announcement was significant in two ways. First, it represents the first such announcement by a member of the Google Books collaboration. Second, the announcement underscores a growing trend in digital library development&#x2014;widespread harvestability of a project&#x2019;s digital items and its metadata. Announcements such as these represent a boon for libraries and their patrons. As more collections move into the digital space, library patrons cannot help but benefit. However, for library technical services offices, announcements such as this can present new challenges. This paper considers options for handling these challenges by focusing on one tool, MarcEdit (http://oregonstate .edu/~reeset/marcedit).</p><p>As more digital services like the UM Library Google Books project move their metadata into the public Web space, library technical services departments need to determine how they will make use of this new influx of available metadata. For sure, some libraries have become accustomed to the many issues dealing with non&#x2013;MARC metadata within what is still largely a MARC&#x2013;centric universe. For libraries hosting digital collections or institutional repositories, challenges related to the representation of those digital objects within a library&#x2019;s many discovery tools like the OCLC Online Computer Library Center&#x2019;s (OCLC) WorldCat or local integrated library system (ILS) are commonplace. While most digital collection software (for example, DSpace, Fedora, and CONTENTdm) and many vendor product solutions (like NewsBank&#x2019;s Congressional Serials Set) provide the ability to harvest item metadata by using OAI-PMH, few libraries use these metadata streams to generate MARC records. The process of downloading, converting, and managing metadata records beyond the traditional MARC metadata workflow remains largely unexplored in many libraries. For those that do repurpose non&#x2013;MARC metadata in some way, the process is often limited to a single service or metadata stream. For example, both Texas A&amp;M University Libraries and the University of Virginia (UV) Library documented their efforts to develop site-specific metadata harvesters for converting bibliographic metadata for electronic theses and dissertation records submitted to their institutional repositories into MARC.</p><p>Non&#x2013;MARC metadata models for sharing digital metadata are not likely to disappear, and technical services departments will need to adjust to new forms of metadata acquisition. During the past twenty years, OCLC and the Library of Congress (LC) have provided libraries with a single, centralized metadata repository from which to download bibliographic metadata. While OCLC remains the largest database of available bibliographic content, the actual distribution of metadata today is becoming much more decentralized. Institutional repositories and digital collection software have played a role in moving the library from metadata consumers and creators to metadata distributors. For libraries looking to leverage content housed in digital collections, understanding and developing processes of harvesting and converting non&#x2013;MARC metadata will be essential for moving forward</p><p>Together, the Open Archive Initiative (OAI, www.openarchives .org) and library communities have worked in recent years to provide a number of tools to facilitate the harvesting and conversion of OAIPMH&#x2013;compliant metadata into other delivery formats, both non&#x2013;MARC and MARC. Traditionally, these tools have been released as parts of &#x201C;kits&#x201D; or components that library developers could use in specialized conversion tools. However, while these tools and kits have provided library information technology (IT) departments greater access to bibliographic metadata, they have done little to help technical services departments deal with OAI-PMH data. More recently, OCLC released an updated version of its Connexion software that provides limited capabilities for metadata harvesting of up to one hundred records through OAI-PMH; the software supports various flavors of Dublin Core (DC). This is a step in the right direction, but it provides no flexibility for customizing the data conversion itself, thus making record creation a one-size-fits-all process. The flexible nature of non&#x2013;MARC metadata formats coupled with the lack of a formal standard for inputting metadata within non&#x2013;MARC formats has made metadata creation somewhat uneven and not easily managed using a generic conversion process. The issue is well known in cataloging circles, as noted in an article found in Online Libraries and Microcomputers. 3 Here the author notes the many challenges one encounters when attempting to crosswalk metadata from one format to another. The one-size-fits-all approach to metadata is problematic because of issues related to granularity and consistency. Crosswalking metadata from one level of granularity to another is always difficult. For example, when moving from a schema of high granularity like MARC to a less granular schema like DC, the loss of both bibliographic content as well as context is often unavoidable. For instance, MARC 21 has numerous fields to represent the &#x201C;author&#x201D; of an item with each field containing contextual information about that &#x201C;author.&#x201D; In unqualified DC, this context and granularity is lost because all &#x201C;authors&#x201D; are placed into a single dc:author element. Likewise, metadata of lower granularity cannot easily be moved to schemas with higher granularity because context and content cannot be manufactured if it is not present within the original record. Second is the issue of consistency. Although all DSpace and CONTENTdm software platforms use DC as the method for primary markup, the best practices used when generating metadata vary widely, potentially varying between projects within a single institution. The lack of a national standard or shared best practices when creating non&#x2013;MARC metadata has contributed to a high level of inconsistency in the metadata currently being produced. This inconsistency makes capturing subtle relationships expressed within the metadata difficult and can result in overly broad and only marginally useful MARC records generated using these generic translation processes. </p><p>Seeing a need for a process that both flexibly and reliably converts metadata from OAI-PMH metadata stores into bibliographic formats usable by its online catalog, Oregon State University (OSU) Libraries chose to use MarcEdit, a freely available client application (developed by the author) that offers default conversion support from OAI-PMH metadata to a number of different metadata formats. This paper will provide a brief discussion of MarcEdit&#x2019;s metadata harvesting functionality as well as provide a detailed description of two potential use cases. The first example details how OSU Libraries catalogers use MarcEdit to harvest unqualified DC metadata from electronic theses in its institutional repository and automatically generate MARC 21 records for inclusion into both the online catalog and WorldCat. Through this conversion process, OSU Libraries has been able to avoid expensive effort duplication and, more importantly, has developed a simple workflow that can be used by technical services staff to capture OAI-PMH metadata from any OAI-PMH provider and generate records for the OSU Libraries catalog or OCLC. The second example demonstrates how staff can generate MARC records from the UM Library Google Books metadata. The process will detail some of the problems that can be encountered while working with metadata from remote metadata repositories as well as ways of overcoming those challenges.</p></sec><sec><title>Literature Review </title><p>Given the pervasiveness of Extensible Markup Language (XML)&#x2013;based metadata and the wide range of protocols that support and advertise the presence of available metadata, it is surprising that automated metadata harvesting, MARC record generation, and library staff&#x2013;centric tools development is not more frequently addressed in the literature. Several articles detail the process of indexing and harvesting MARC data into other indexing systems like Solr (http:// lucene.apache.org/solr). Likewise, tools like Villanova&#x2019;s VuFind (www .vufind.org) and UV Library&#x2019;s Project Blacklight (http://blacklight.betech .virginia.edu) have advanced discussions relating to MARC indexing outside of a non&#x2013;MARC environment. Only a few articles discuss processes for reusing XML&#x2013;based metadata formats in MARC environments, and fewer still have been written specifically for technical services staff. Most have concentrated on the potential for reusing existing metadata in one&#x2019;s institutional repository to generate MARC records for submitted electronic theses and dissertations.</p><sec><title>ETD2Marc</title><p>Surratt and Hill&#x2019;s article on the development of a customized ETD2MARC processing documented how Texas A&amp;M University Libraries was able to customize a process developed by UV Library to provide a semiautomated record generation tool. Integrated into their workflow, the tool provided a way for staff to automatically generate MARC records for items as they were submitted into their institutional repository.4 The resulting files from the metadata translation were dirty, core-level MARC records, which were then reviewed and edited by a staff member and finally entered in the online catalog and sent to OCLC. Texas A&amp;M University Libraries&#x2019; conversion script allowed their catalogers to more efficiently process electronic theses and dissertations (ETDs) by making use of attached metadata. While the article provided a copy of the script used to perform the conversion process, little evidence suggests that other institutions were able to use the Texas A&amp;M University Libraries&#x2019; method to promote metadata repurposing at their own institutions. The reason lies in the implementation. The process documented by Surratt and Hill fulfills the needs of the organization but is so tightly coupled to the organization&#x2019;s workflow that it becomes unusable without significant revision when taken outside of that environment. In addition, the process of data conversion was moved outside of technical services, meaning that a firewall was placed between the catalogers and the developers that created the script.</p><p>An article by Kurth, Ruddy, and Rupp documents an ongoing metadata repurposing project at the Cornell University (CU) Library. Unlike the process documented by Surratt and Hill, the CU Library project looks at the development of a service to repurpose MARC metadata for use within one&#x2019;s digital library infrastructure.</p><p>Kurth, Ruddy, and Rupp note that metadata currently found within the online catalog could be used to enrich many of the digital services and projects at CU Library. However, to use this metadata, a system needed to be developed that broke down MARC metadata and reassembled it for use in the Text Encoding Initiative (TEI) and DC. What makes this system interesting is the cooperative relationship between CU Library&#x2019;s metadata services and its IT department. While the article notes that the IT department develops and maintains the MARC processing scripts and document type definitions (DTD) for validation and creates the Extensible Stylesheet Language Transformations (XSLTs) used to crosswalk MARCXML data to TEI or DC, the collection-specific MARC mappings were created in conjunction with stakeholders from within the library. Since metadata conversions feed metadata directly to specific digital projects, the conversion must be completely automated. In this case, that is possible because of the controlled nature of the metadata and the granularity of the destination metadata schema. </p><p>A 2005 article by this author in the Journal of Map and Geography Libraries described the process used by OSU Libraries to generate MARC records for Geographic Information Systems (GIS) datasets from the accompanying Federal Geographic Data Committee (FGDC) metadata records.6 Using MarcEdit, OSU Libraries was able to create a generic XSLT stylesheet that could be used as a template for translating FGDC metadata to MARC 21 XML. Once in MARC 21 XML, MarcEdit is able to translate the metadata into MARC 21 as well as accommodate character set translations between the legacy MARC- 8 and more current 8-bit Unicode Transformation Format (UTF-8). Because of the richness of data found within the FGDC data format, the MARC records generated from the FGDC data sources often included much more detailed information than records generated without the FGDC metadata. Although this process is not fully automated because records are not harvested and translated automatically, the process is portable.</p></sec><sec><title>Available Tool Sets</title><p>Presently, a number of effective development toolsets and software kits exist to provide OAI functionality to library tools. Developers interested in working with the OAI-PMH protocol are able to choose from components developed in a variety of languages, such as the Perl OAI modules, the Ruby OAI gem, or one of the many Java OAI harvesting kits; components have been readily available for some time for developers looking to build resources to aggregate metadata together. The OAI keeps track of a number of usercontributed tools and toolkits.</p><p>paper is to look at resources that enhance technical services staff&#x2019;s ability to take advantage of existing metadata, not to examine resources developed for the developer community. While a rich ecosystem of developer-related tools exists for processing OAI-PMH metadata, these tools provide very </p></sec></sec><sec><title>Innovative Interfaces&#x2019; XML Harvester</title><sec><title>Available Tool Sets</title><p>Presently, a number of effective development toolsets and software kits exist to provide OAI functionality to library tools. Developers interested in working with the OAI-PMH protocol are able to choose from components developed in a variety of languages, such as the Perl OAI modules, the Ruby OAI gem, or one of the many Java OAI harvesting kits; components have been readily available for some time for developers looking to build resources to aggregate metadata together. The OAI keeps track of a number of usercontributed tools and toolkits.</p><p>paper is to look at resources that enhance technical services staff&#x2019;s ability to take advantage of existing metadata, not to examine resources developed for the developer community. While a rich ecosystem of developer-related tools exists for processing OAI-PMH metadata, these tools provide very </p><p>Presently, many vendors have or are developing tools to facilitate the harvest of non&#x2013;MARC metadata into the online catalog. ILS vendors like Innovative Interfaces have moved to create systems to streamline metadata harvesting directly into the online catalog. The Innovative Interfaces metadata solution known as XML Harvester, developed in cooperation with Michigan State University (MSU), is representative of most ILS vendor-supplied data harvesting tools because it provides one-way metadata conversion from a single data source into the online catalog. XML Harvester was used initially by MSU to generate MARC records in the online catalog from harvested EAD metadata, although today it can provide conversions from a number of different metadata formats.</p><p>XML Harvester&#x2019;s functionality is representative of most ILS vendorsupplied metadata harvesting applications. Since this class of applications tends to run at the server level, control over how metadata crosswalking is defined will vary in granularity and generally be available only to IT staff or those at the system level. Likewise, this class of tools tends to be designed to be single project solutions, meaning that a significant amount of time is generally required for set up and testing to harvest a single collection, overhead that must be reallocated each time a new collection is set to be harvested. Because translations are tailored to specific projects or collections, work done for one project cannot be shared or used when looking to harvest other collections. This places practical limits on the types of projects that these tools can support. While the tight coupling with the ILS generally simplifies the process of loading and updating harvested metadata, it does come at a price. XML Harvester, for example, can only be used to harvest metadata into the online catalog and Encore Platform rather than as an abstract harvesting tool for providing metadata conversion services. This does tend to put very specific limits as to how useful this class of tools can be in general, particularly when considering the wide range of databases and services library technical service departments are being asked to maintain. The ability to harvest metadata and convert it into many different formats will likely become more important with time, possibly shortening the shelf life for this class of applications.</p></sec></sec></body><back><ref-list><ref><note><p>This is a citation</p></note></ref><ref><note><p>This is a second citation</p></note></ref></ref-list></back></article>

