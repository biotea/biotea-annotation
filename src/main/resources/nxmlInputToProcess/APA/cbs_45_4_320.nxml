<?xml version='1.0' encoding='US-ASCII'?>
<!DOCTYPE article PUBLIC "-//APA//DTD APA Journal Archive DTD v1.0 20130715//EN" "http://xml.apa.org/serials/jats-dtds-1.0/APAjournal-archive.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" xml:lang="en" structure-type="article" dtd-version="1.0">
<front>
<journal-meta>
<journal-title-group>
<journal-title xml:lang="en">Canadian Journal of Behavioural Science/Revue canadienne des sciences du comportement</journal-title></journal-title-group>
<issn pub-type="print">0008-400X</issn>
<issn pub-type="online">1879-2669</issn>
<publisher>
<publisher-name>Educational Publishing Foundation</publisher-name></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="apaID">cbs_45_4_320</article-id>
<article-id pub-id-type="doi">10.1037/a0033357</article-id>
<article-id pub-id-type="pi-uid">2013-38228-004</article-id>
<article-categories>
<subj-group subj-group-type="toc-heading">
<subject>Regular Articles / Articles r&#233;guliers</subject>
</subj-group>
</article-categories>
<title-group><article-title>Evaluating the Equivalence of, or Difference Between, Psychological Treatments: An Exploration of Recent Intervention Studies</article-title>
</title-group>
<contrib-group content-type="journal-editors"><contrib contrib-type="editor" corresp="no" xlink:type="simple"><string-name><given-names>Todd G.</given-names> <surname>Morrison</surname></string-name> <role>Editor</role></contrib> </contrib-group>
<contrib-group content-type="primary-authors">
<contrib rid="aff1 corr1" contrib-type="author" corresp="yes" xlink:type="simple">
<string-name>
<given-names>Teresa A.</given-names> <surname>Allan</surname></string-name>
</contrib>
<contrib rid="aff1" contrib-type="author" corresp="no" xlink:type="simple">
<string-name>
<given-names>Robert A.</given-names> <surname>Cribbie</surname></string-name>
</contrib>
<aff id="aff1">Quantitative Methods Program, Department of Psychology, York University, Toronto, Ontario, Canada</aff>
</contrib-group>
<author-notes>
<corresp id="corr1"><addr-line>Teresa A. Allan</addr-line> <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="live" xlink:href="mailto:TeresaAllan@rogers.com" ext-link-type="email" xlink:type="simple">TeresaAllan@rogers.com</ext-link></corresp>
</author-notes>
<pub-date pub-type="print"><string-date><month number="10">October</month> <year>2013</year></string-date></pub-date>
<volume>45</volume>
<issue>4</issue>
<fpage>320</fpage>
<lpage>328</lpage>
<history>
<string-date content-type="received"><month number="9">September</month> <day>14</day>, <year>2012</year></string-date>
<string-date content-type="revised"><month number="5">May</month> <day>7</day>, <year>2013</year></string-date>
<string-date content-type="accepted"><month number="5">May</month> <day>9</day>, <year>2013</year></string-date>
</history>
<permissions copyright-status="active">
<copyright-statement>&#169; 2013 Canadian Psychological Association</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder>Canadian Psychological Association</copyright-holder>
</permissions>
<abstract xml:lang="en">
<p align="left">In behavioral science research there is often the need to determine if an outcome variable differs, or is equivalent, across groups. Significance tests are the most prevalently applied data analysis method for this type of question. The purpose of this study was to examine how statistical tests for equivalence and difference have been applied to compare clinical interventions. Peer-reviewed journal articles that made treatment comparisons were examined. For each study, the primary hypothesis, statistical test usage, and the stated conclusion were recorded. Of the 270 studies investigated, 54.4% inappropriately made equivalence-based conclusions from difference-based test statistics (e.g., <italic>t</italic> test, ANOVA). Significance tests are often applied as a matter of course regardless of the research question. We have found that difference tests are similarly favored and have been applied to examine difference and inappropriately applied to examine equivalence. We discuss our findings and provide resources for researchers who want to statistically evaluate between-groups equivalence.</p>
</abstract>
<abstract xml:lang="fr">
<p align="left">Dans la recherche en science du comportement, il est souvent n&#233;cessaire de d&#233;terminer si une variable de r&#233;sultat diff&#232;re ou est &#233;quivalente parmi des groupes. Des tests de signification constituent la m&#233;thode d&#8217;analyse des donn&#233;es la plus utilis&#233;e pour r&#233;pondre &#224; cette question. Cette &#233;tude avait pour but d&#8217;examiner comment les analyses statistiques visant &#224; d&#233;terminer l&#8217;&#233;quivalence ou la diff&#233;rence ont &#233;t&#233; appliqu&#233;es pour comparer des interventions cliniques. Des articles de revues ayant &#233;t&#233; &#233;valu&#233;s par des pairs et comportant des comparaisons de traitements ont &#233;t&#233; examin&#233;s. Pour chaque &#233;tude, on a consign&#233; l&#8217;hypoth&#232;se principale, l&#8217;usage d&#8217;une m&#233;thode statistique et la conclusion &#233;nonc&#233;e. Parmi les 270 &#233;tudes de l&#8217;&#233;chantillon, 54,4 % arrivaient &#224; des conclusions erron&#233;es d&#8217;&#233;quivalence au moyen de m&#233;thodes statistiques reposant sur la variance (par ex., test t, ANOVA). On a souvent recours &#224; des tests de signification, de fa&#231;on routini&#232;re, peu importe le sujet de la recherche. Nous avons conclu que les tests de variance sont aussi privil&#233;gi&#233;s et qu&#8217;ils ont &#233;t&#233; utilis&#233;s pour examiner des diff&#233;rences et mal appliqu&#233;s pour analyser l&#8217;&#233;quivalence. Nous discutons de nos r&#233;sultats et citons des ressources &#224; l&#8217;intention de chercheurs qui veulent &#233;valuer d&#8217;un point de vue statistique l&#8217;&#233;quivalence entre des groupes.</p>
</abstract>
<kwd-group xml:lang="en">
<kwd>equivalence tests</kwd>
<kwd>statistical equivalence</kwd>
<kwd>clinical interventions</kwd>
<kwd>treatment comparisons</kwd>
</kwd-group>
<kwd-group xml:lang="fr">
<kwd>tests d&#8217;&#233;quivalence</kwd>
<kwd>&#233;quivalence statistique</kwd>
<kwd>interventions cliniques</kwd>
<kwd>comparaison de traitements</kwd>
</kwd-group>
</article-meta></front>
<body>
<sec id="s1">
<p align="left">In the behavioral sciences, researchers often need to compare two or more psychological treatments. Some researchers explore their questions qualitatively, and others explore them quantitatively. Within the quantitative domain, there are many methodologies and schools of thought on which mathematical process is the best procedure to apply in order to answer a given research question. Among those many methods are significance tests, Bayesian methods, and confidence intervals. Instead of entering into the debate of which method is the best, the present study will only focus on what is occurring in our field when researchers apply the most controversial, yet most used method: statistical tests of significance (<xref ref-type="bibr" id="cr17-1" rid="c17">Kline, 2004</xref>). We will, however, elaborate briefly on alternatives to significance testing and provide resources for further reading.</p>
<p align="left">In some investigations, the researchers&#8217; objective is to determine whether the observed differences across treatment conditions are significant. Thus they would select a null-hypothesis of &#8220;no difference,&#8221; hoping that the result of their test would be a rejection of the null, which would support the presence of the differential effect that they hypothesized they would find. In others, the aim is to determine if the treatments of interest are equivalent, in which case a null hypothesis of nonequivalence would be selected, and a rejection of that null would support treatment equivalence (within the predefined range for clinical equivalence). If researchers are using significance testing, a test can be applied to compare their data to an established distribution to see if they can reject the null, and in a perfect world, support their hypothesis. As predictions relating to difference and those relating to equivalence are distinct hypotheses, the tests of statistical significance that appropriately address them also differ. Thus, we have significance tests for difference (e.g., Student&#8217;s <italic>t</italic> test and ANOVA) and significance tests for equivalence (e.g., Schuirmann&#8217;s Two-One-Sided Test [TOST] procedure and Wellek&#8217;s noncentral <italic>F</italic> procedure; <xref ref-type="bibr" id="cr27-1" rid="c27">Schuirmann, 1987</xref>; <xref ref-type="bibr" id="cr36-1" rid="c36">Wellek, 2010</xref>.</p>
<p align="left">Methods of statistically testing for difference are familiar to most of us because these tests are presented in nearly every introductory statistics course, textbook, and statistical software package. Researchers learn during their undergraduate years to apply these tests in an attempt to answer the question: <italic>Are these groups (or treatments) different?</italic> Alternately, equivalence tests, can be applied when the research question is: <italic>Are these groups (or treatments) equivalent?</italic> (<xref ref-type="bibr" id="cr6-1" rid="c6">Cribbie, Gruman, &amp; Arpin-Cribbie, 2004</xref>; <xref ref-type="bibr" id="cr25-1" rid="c25">Rogers, Howard &amp; Vessey, 1993</xref>; <xref ref-type="bibr" id="cr36-2" rid="c36">Wellek, 2010</xref>). Equivalence tests, however, are not included in introductory statistics courses, textbooks, and most statistical software packages and thus are less popular.</p>
</sec>
<sec id="s2">
<title>What Difference Tests Can and Cannot Do</title>
<p align="left">Because difference tests can only provide a yes/no answer to the question: <italic>Is there a statistically significant difference between these groups of data?</italic> These tests are neither able to detect, nor to provide any useful information about equivalence. Failing to detect a statistically significant difference between groups is not the same as establishing that the treatments or groups are equivalent on that given measure (<xref ref-type="bibr" id="cr4-1" rid="c4">Cribbie, Arpin-Cribbie &amp; Gruman, 2009</xref>; <xref ref-type="bibr" id="cr10-1" rid="c10">Gordon, 1985</xref>; <xref ref-type="bibr" id="cr17-2" rid="c17">Kline, 2004</xref>; <xref ref-type="bibr" id="cr27-2" rid="c27">Schuirmann, 1987</xref>; <xref ref-type="bibr" id="cr31-1" rid="c31">Stegner, Bostrom, &amp; Greenfield, 1996</xref>; <xref ref-type="bibr" id="cr34-1" rid="c34">Tryon &amp; Lewis, 2008</xref>; <xref ref-type="bibr" id="cr37-1" rid="c37">Westlake, 1976</xref>). To conclude equivalence or nonequivalence from a difference test is a logical error that equates to saying verbally, <italic>I failed to find a difference in my set of data; therefore, a difference does not exist, and these treatments must be equivalent</italic> (See <xref ref-type="bibr" id="cr3-1" rid="c3">Cohen, 1994</xref>, for a more detailed discussion on hypothesis testing logic). If we look for an effect, and fail to find it, it is not appropriate to conclude that the effect does not exist. A real-world analogy that we might apply here to clarify this idea is to think about hypothesis testing logic in the context of a courtroom. Failing to find guilt is not the same as proving innocence, even though guilt and innocence seem to be semantically opposed in our everyday vocabulary. Like guilt and innocence, difference and equivalence are separate questions: failing to find one of these is not sufficient evidence to conclude the presence of the other.</p>
<p align="left">At the level of the sample, difference tests can only provide binary answers to the question: <italic>In the sets of data that I have collected, is there a significant difference between treatment outcomes for these two (or more) groups?</italic> To determine this, a difference significance test, such as a Student&#8217;s <italic>t</italic> test, can be run on the researcher&#8217;s data. The results of that test are binary: significance or nonsignificance. To verbalize these ideas more plainly, a finding of significance is a statement: <italic>Yes, there is a significant difference between these groups of data, where H<sub location="post">0,</sub> the null hypothesis of &#8220;no difference,&#8221; has been rejected.</italic> Conversely, <italic>No, based upon my data, there is not a statistically significant difference between these groups on this measure</italic>, could be a simplified verbal representation for the case when the test yields a &#8220;failure to reject the null hypothesis.&#8221; When difference tests are adequately powered, most are effective tools for determining if even very small differences between groups are statistically significant.</p>
</sec>
<sec id="s3">
<title>What Equivalence Tests Can and Cannot Do</title>
<p align="left">Because difference tests are an appropriate avenue within significance testing to detect statistically significant differences between treatments, equivalence tests are similarly specific to the task of detecting clinical equivalence (<xref ref-type="bibr" id="cr31-2" rid="c31">Stegner et al., 1996</xref>). Let us first distinguish equality from clinical equivalence (which from here on, we will simply refer to as equivalence). Equal is, as it sounds, exactly identical central tendencies. Therefore, in reality, what we might want to know is: Are these two treatments similar enough that I can recommend the shorter, less costly, or less invasive treatment to my client and have my client get the same beneficial result as the lengthier, more expensive, or more invasive treatment?</p>
<p align="left">In order to examine equivalence, we must identify a range that represents inconsequential difference: an equivalence interval (<xref ref-type="bibr" id="cr25-2" rid="c25">Rogers et al., 1993</xref>). Imagine a situation where we might consider treatments to be equivalent if posttreatment test scores were within &#177;5 points on our valid and reliable outcome measure. This would mean that we would consider a score of 80 points to be clinically equivalent to a score of 85. Providing that this difference is small enough to be considered inconsequential, that is, within a predefined range of acceptable difference, we would set our equivalence interval to [&#8722;5, 5] to represent that a difference of this magnitude in either direction would be considered meaningless. It is important to point out that equivalence intervals are established as part of the research design a priori and factors such as the attributes of the participant population, reliability, the scale of the outcome variable, and the underlying nature of the study will all affect the size of the equivalence interval (<xref ref-type="bibr" id="cr11-1" rid="c11">Greene, Concato, &amp; Feinstein, 2000</xref>; <xref ref-type="bibr" id="cr25-3" rid="c25">Rogers et al., 1993</xref>). The range of values that will be considered to be clinically equivalent will vary greatly from study to study; thus, it is up to the authors of each study to determine that range and provide evidence to support their selection. Because defining this value is a complex procedure, the interested reader is referred to <xref ref-type="bibr" id="cr25-4" rid="c25">Rogers et al. (1993)</xref> and <xref ref-type="bibr" id="cr36-3" rid="c36">Wellek (2010)</xref> for a more thorough discussion.</p>
<p align="left">Like difference tests, the results of equivalence tests conducted at the level of the sample also provide us only binary answers based on our data. When the hypothetical question is: <italic>Based on the range I have defined, are these groups of data significantly equivalent on this outcome measure?</italic> there are only two logically correct answers that may be gleaned from the results of a significance test for equivalence: <italic>Yes, these groups of data (treatments outcome scores) that I have collected are significantly equivalent</italic> or <italic>No, these groups of data that I have collected are not significantly equivalent.</italic> Given the logic behind a binary (yes/no) question and what can be inferred from it, it is logically incorrect to say: I have failed to find these groups to be significantly equivalent based on my data, therefore a significant difference between them exists. The reason that this statement is incorrect is that a test for equivalence can neither confirm nor refute the presence of difference because difference is not being evaluated.</p>
</sec>
<sec id="s4">
<title>Present Study</title>
<p align="left">The significance testing for equivalence literature cites both examples of correctly and incorrectly applied procedures (<xref ref-type="bibr" id="cr17-3" rid="c17">Kline, 2004</xref>; <xref ref-type="bibr" id="cr36-4" rid="c36">Wellek, 2010</xref>). However, the prevalence of both correct and incorrect conclusions of equivalence, and the tests used to support them, is something that has not been previously assessed in psychological treatment comparisons. The value that the present study brings to the behavioral science field is a description of the prevalence of correct and incorrect implementations of, and conclusions from, difference- and equivalence-based significance test statistics. We also provide several equivalence testing resources for those who want to use significance testing to examine equivalence, and we also present some other (also underutilized) alternatives and associated resources.</p>
<p align="left">The purpose of the present study was to examine years 2000 to 2010 of the psychological literature and provide a detailed description of how significance testing statistics are being used in the behavioral sciences for cases where researchers have compared two or more treatments. Of interest, more specifically, was the prevalence of both appropriate and inappropriate conclusions of clinical difference or equivalence between treatments and how statistical tests for difference and equivalence have been used to support these statements. In addition to forming a simple accounting of how statistical tests have been used in relation to findings of equivalence, we wanted to examine the hypothesis&#8211;test&#8211;conclusion process as a whole from start to finish, per study, to determine the congruence of the overall process. To accomplish this, hypotheses, statistical tests used, and the subsequent conclusions stated were categorized in accordance with the definitions below, and these are further elaborated on in the Methods section.</p>
</sec>
<sec id="s5">
<title>Comparison Tests for Difference</title>
<p align="left">We defined comparison testing for difference as those comparisons in which the researcher had hypothesized that a difference would be found between two or more treatments. A simple instance of this is when a researcher hypothesized that a given treatment would be more effective than a placebo. Difference tests might also be used when there is a need to determine the effectiveness of a treatment as usual (TAU) compared with the TAU plus an additional feature&#8212;essentially to see if the new feature generates a noticeable difference on the outcome measure. A practical example of this is when the TAU for depression was administered to one group&#8212;a type of selective serotonin reuptake inhibitor (SSRI), for example, and results of that drug-only intervention were compared with another group of study participants that received both the SSRI and a new feature: weekly sessions of cognitive behavioral therapy (CBT). Presume that the study used random assignment and a complete data set was obtained for all participants across all time points. If a statistically significant difference were found, favoring the drug plus CBT condition, that finding would suggest that additional therapy sessions may have been clinically useful, provided the difference found represents a meaningful clinical effect. Although a finding such as this indicates that there is a difference between the two treatment groups, it is important to highlight that this conclusion does not provide definitive evidence of the presence of a difference. It is still possible that the statistically significant outcome represents a Type I error, even though the probability of a Type I error is typically set a comfortably low level (e.g., 5%).<xref ref-type="fn" id="fnc1-1" rid="fn1"><sup location="post">1</sup></xref></p>
</sec>
<sec id="s6">
<title>Comparison Tests for Equivalence</title>
<p align="left">In addition to searching for ways to augment existing treatments, it may be of value to researchers and treatment providers to determine if the effects of two differing treatments can be considered to be clinically equivalent. An everyday example of a need to determine if two groups are equivalent on a posttreatment measure is when two formats of administering counseling, either face-to-face or via the Internet, are being evaluated. The research goal, in this case, would usually be to find out if the Internet therapy can provide results equivalent to face-to-face therapy. Additionally, a researcher may want to evaluate treatments for interchangeability. It may also be of value, in some cases, to determine if equivalence exists between a new drug that might be less expensive, or have fewer side effects, than an existing treatment for the same condition (e.g., does it reduce anxiety as well as the earlier formulation?) (<xref ref-type="bibr" id="cr2-1" rid="c2">Anderson &amp; Hauck, 1983</xref>; <xref ref-type="bibr" id="cr15-1" rid="c15">Howland, 2009</xref>). A third instance of when an equivalence test might be a useful tool is when two administration periods of psychotherapy are being evaluated for efficacy and cost-effectiveness. In a study such as this, 12 weeks of therapy might be compared with both an eight- and 10-week program to see if similar benefits can be obtained in a shorter amount of time.</p>
<p align="left">An important distinction between difference-based and equivalence-based tests is how sample size and effect size affect the power of these two types of tests. A traditional difference-based test gains power for detecting differences as the sample size and mean difference increase (e.g., power for detecting differences goes up when we increase the sample size of each group from 50 to 100 participants, and power also goes up when the differences in the means of the groups is increased from 5 to 10 points). On the other hand, for equivalence tests, power for detecting equivalence increases as sample sizes increase and mean differences decrease. It is important for the power of equivalence tests to increase with sample size in order for the tests to be consistent with the principles of null hypothesis testing and for the power to detect equivalence to increase as the effect size decreases, because the tests are designed to detect a situation in which there is very little difference in the means.</p>
</sec>
<sec id="s7">
<title>Method</title>
<sec id="s8" disp-level="subsect1">
<title>Studies Examined</title>
<p align="left">The studies examined in this study were collected from the PsycINFO database using a keyword searching procedure. The Boolean search phrase: ti(&#8220;vs.&#8221; OR &#8220;vs.&#8221; AND &#8220;treatment*&#8221; or &#8220;therapy&#8221;) was used in the PsycINFO advanced search field. (The &#8220;ti&#8221; outside of the parentheses is a shortcut to tell PsycINFO to apply this Boolean phrase to the publication title field.) The search was limited to peer-reviewed journal articles published between January 1, 2000, and December 31, 2010, and yielded over 1,000 results.</p>
<p align="left">In addition to the requirement that the study must be published in English, two selection criteria were used. First, the study compared treatments for a diagnosable behavioral condition (e.g., anxiety, depression, phobias) or compared two or more treatments for a measurable psychological factor (e.g., interventions to improve one&#8217;s perceived quality of life). Thus, studies that examined purely medical treatments (e.g., cardiac medication comparisons, or comparisons of treatments for broken bones) were excluded. To determine if a study examined &#8220;a measurable psychological factor,&#8221; we operationally defined a measurable psychological factor as one that was obtained through the use of a psychological inventory (e.g., Minnesota Multiphasic Personality Inventory, Beck Depression Inventory, or a novel psychological measurement scale). Studies that were not selected because they did not meet this criterion included those that measured bone density as an evaluation of treatments for osteoporosis, those that used electrocardiographs to evaluate heart medications, and similar purely medical investigations. This criterion was also used to exclude single case studies. Second, The study used original data. This criterion excluded meta-analyses, which prevented the collection of redundant data. To determine if a study used &#8220;original data,&#8221; we operationally defined original data studies as those that were not meta-analyses or compilations/reviews of previous works. Also, to meet this criterion, a study needed to indicate how data were collected. Studies that did not clearly state that the collection of new/unique data took place were not included in our sample.</p>
<p align="left">This selection process yielded 270 current, peer-reviewed psychological comparison studies from 106 journals. Of these, there were 139 therapy studies (therapy vs. therapy, <italic>n =</italic> 97; therapy administration methods compared, <italic>n =</italic> 31; and therapy vs. placebo, waitlist, or TAU, <italic>n = 11</italic>), 113 pharmaceutical studies (drug vs. drug, <italic>n =</italic> 95; drug vs. placebo, <italic>n =</italic> 12; and six were additional variants of this, some including an additional treatment of interest or waitlist control group), and 18 studies that combined differing variants of drug versus psychotherapy. An average of 24.5 studies per publication year were collected (<italic>n =</italic> 17, 20, 18, 29, 15, 29, 40, 19, 26, 29, and 28, respectively, from the years 2000 to 2010).</p>
</sec>
<sec id="s9" disp-level="subsect1">
<title>Procedure</title>
<sec id="s10" disp-level="subsect2">
<title>Categorization of significance tests</title>
<p align="left">Common examples of difference tests are Student&#8217;s <italic>t</italic> test, ANOVA, chi-square, and Fisher&#8217;s exact test, and examples of equivalence tests are Schuirmann&#8217;s TOST, Wellek&#8217;s Test, and tests that use confidence interval approaches alongside significance tests to detect equivalence (<xref ref-type="bibr" id="cr27-3" rid="c27">Schuirmann, 1987</xref>; <xref ref-type="bibr" id="cr28-1" rid="c28">Seaman &amp; Serlin, 1998</xref>; <xref ref-type="bibr" id="cr34-2" rid="c34">Tryon &amp; Lewis, 2008</xref>; <xref ref-type="bibr" id="cr36-5" rid="c36">Wellek, 2010</xref>; <xref ref-type="bibr" id="cr37-2" rid="c37">Westlake, 1976</xref>). The statistical tests that were used in each study were categorized as Difference Tests and/or Equivalence Tests based on the null hypothesis (i.e., null hypotheses of no difference were categorized as difference tests and null hypotheses of nonequivalence were categorized as equivalence tests). Studies that did not state the use of significance tests or provide the results of any mathematical comparisons were categorized accordingly.</p>
</sec>
<sec id="s11" disp-level="subsect2">
<title>Categorization of hypotheses and conclusions</title>
<p align="left">For our purposes, studies that specifically used verbiage indicating that the study objective was to determine if two treatments or groups were &#8220;equivalent,&#8221; &#8220;equal,&#8221; &#8220;comparable,&#8221; &#8220;similar,&#8221; &#8220;as effective as&#8221; (or some combination of these terms) in their hypotheses or &#8220;purpose of the study&#8221; statements were categorized as intending to examine equivalence. Concluding phrases stating that the treatment(s) of interest &#8220;are both effective,&#8221; &#8220;is as effective as,&#8221; &#8220;is an alternative to,&#8221; &#8220;are comparable,&#8221; &#8220;are similar,&#8221; the &#8220;new treatment is equal to old treatment,&#8221; &#8220;are equal&#8221;/&#8220;equally effective,&#8221; and &#8220;are equivalent&#8221; were categorized as a conclusion relating to the state of equivalence between the treatments or therapies (see <xref ref-type="fig" id="fgc1-1" rid="fig1">Figure 1</xref>).<xref ref-type="fig-anchor" rid="fig1"/></p>
<p align="left">Hypotheses that specifically predicted difference, either directional or nondirectional, were classified as intending to examine difference. Conclusions that only stated that there was &#8220;no significant difference&#8221; between treatments, or conclusions that only stated that &#8220;a difference&#8221; or &#8220;significant difference&#8221; was found were coded as conclusions relating to the state of difference between groups. If hypotheses were unstated, exploratory, or minimal and nonpredictive (e.g., &#8220;our study <italic>evaluated</italic> Drug A vs. Drug B for the treatment of &#8230;&#8221;), these were classified as &#8220;intending to conduct comparisons.&#8221; Conclusions that did not specifically conclude difference or equivalence as a result of the study were few and were categorized as descriptive or inconclusive.</p>
</sec>
</sec>
<sec id="s12" disp-level="subsect1">
<title>Congruence</title>
<p align="left">Three types of congruence were examined: Hypothesis-Test Congruence, Test-Conclusion Congruence, and Overall Congruence (Hypothesis-Test-Conclusion). When hypothesis types matched test types (e.g., difference was hypothesized, and a difference test was used) this was coded as Hypothesis-Test congruence. Conversely, when the hypotheses predicted equivalence and evaluated that prediction with a statistical difference test, this was coded as Hypothesis-Test incongruence. Test-Conclusion Congruence was evaluated similarly: for example, those that conducted difference tests and stated a conclusion relating to the state of difference that was found (or not found) were coded as congruent, and those that conducted difference tests and used them to formulate conclusions regarding a state of equivalence were coded as incongruent. Finally, Overall Congruence was defined as the state where all three (hypothesis, test, and conclusion) matched. For example, a difference hypothesis, examined by a difference test, followed by a conclusion regarding the state of difference was congruent (e.g., <italic>we predict that treatment A will be better than treatment B on this given measure</italic>, tested this using a <italic>t</italic> test, and concluded that a significant difference was found). Studies were only coded as having overall congruence if the hypothesis, test, and conclusion were consistent (difference-difference-difference, or equivalence-equivalence-equivalence). (Further details about this coding process can be found in the <xref ref-type="app" id="apcA-1" rid="A">Appendix</xref>).</p>
<p align="left">For each article, the above variables were recorded by the first author, and subjective decisions were decided jointly by both of the authors. In a small number of cases where the study was ambiguous to both authors in terms of the type of testing procedure that was used or the study did not state a clear hypothesis or conclusion, categories of &#8220;unclear&#8221; and &#8220;not stated&#8221; were used. As noted in the results section, those studies were not included in analyses where these variables were necessary.</p>
</sec>
</sec>
<sec id="s13">
<title>Results</title>
<sec id="s14" disp-level="subsect1">
<title>Hypotheses, Tests, and Conclusions</title>
<p align="left">Of the 270 studies examined, it was found that 116 (43%) provided specific predictive hypotheses. Of those providing specific hypotheses, 91 predicted that a significant difference between treatments would be found, and 25 studies predicted that the treatments of interest would be equivalent (see <xref ref-type="table" id="tbc1-1" rid="tbl1">Table 1</xref>). The remaining 57% (<italic>n</italic> = 154) stated no specific hypothesis, stating only that comparisons were being made, treatments were being evaluated, or the objective of their study was described in terms of Treatment A &#8220;versus&#8221; Treatment B and made no outcome predictions. Two hundred sixty-five studies used difference tests to evaluate their hypotheses. Two studies used equivalence tests. Five studies did not state that any type of mathematical analysis was used to test their hypotheses and thus provided no statistical data. (It could be that these researchers did not apply statistical tests or applied tests but did not mention them in their publications.) It must also be noted that two of the studies used both an equivalence test and a difference test to evaluate the same set of data.<xref ref-type="table-anchor" rid="tbl1"/></p>
<p align="left">Forty percent of the studies we examined (<italic>n =</italic> 109) stated conclusions of difference. Twenty of these used difference tests and correctly stated in the study conclusion that there was &#8220;no significant difference&#8221; between groups, or that &#8220;no differences were found,&#8221; and the remaining 89 correctly concluded that statistically significant differences were found. Seven studies provided conclusions in the form of a descriptive, where treatments were described through the provision of observed frequencies or in terms of pros and cons rather than in terms of equivalence or difference. Two studies stated that based on their statistical findings, their studies were inconclusive at this time and warranted further research.</p>
<p align="left">One hundred forty-seven (54.4%) stated conclusions of equivalence (see <xref ref-type="fig" id="fgc1-2" rid="fig1">Figure 1</xref>). Two studies used equivalence tests to conclude difference between treatments. One of these used both an equivalence test and a difference test on their primary outcome variable first to detect difference and then applied an equivalence test to (inappropriately) confirm it. The other study used an equivalence test to check for differences that an initially applied difference test &#8220;might have missed.&#8221; That study found no significant difference and also failed to find significant equivalence and subsequently concluded that because there was no equivalence, there may be a difference between the treatments that the difference test failed to detect (see <xref ref-type="table" id="tbc1-2" rid="tbl1">Table 1</xref>).</p>
</sec>
<sec id="s15" disp-level="subsect1">
<title>Congruence</title>
<p align="left">Three types of congruence were examined: Hypothesis-Test Congruence, Test-Conclusion Congruence, and Overall Congruence (Hypothesis-Test-Conclusion) (see the <xref ref-type="app" id="apcA-2" rid="A">Appendix</xref>).</p>
<sec id="s16" disp-level="subsect2">
<title>Hypothesis-Test Congruence</title>
<p align="left">Hypothesis-Test Congruence could not be calculated for the 154 studies (57% of the overall sample) that stated their hypotheses as comparisons. Of the 115 studies that provided specific hypotheses and used statistical testing to evaluate their primary hypothesis, 89 of them were congruent with the applied statistical test, and 26 were not. The 89 that were congruent all hypothesized that a difference would be found between the treatments of interest and used difference tests to determine if that difference was statistically significant. Twenty-five that were incongruent hypothesized equivalence and used difference tests to determine if the treatments were equivalent (the study that used an equivalence test to look for differences that the difference test might have missed was also coded as incongruent). Of the cases that used statistical testing to test stated hypotheses, all but two of those that hypothesized difference appropriately tested for difference, and no cases hypothesizing equivalence statistically tested for equivalence. Thus, 97.8% of the studies that stated hypotheses relating to difference correctly adopted a test of difference, and 0% of the studies that stated hypotheses relating to equivalence correctly adopted tests of equivalence.</p>
</sec>
<sec id="s17" disp-level="subsect2">
<title>Test-Conclusion Congruence</title>
<p align="left">Test-Conclusion Congruence could not be calculated for 14 studies&#8212;seven of these provided only descriptive conclusions, two were inconclusive, and five did not indicate the use of statistical testing. The remaining 256 used statistical tests and provided either equivalence conclusions (<italic>n =</italic> 147) or difference conclusions (<italic>n</italic> = 109). One hundred seven of these were congruent with the statistical tests used, and 149 were not. One hundred forty-seven of the 149 studies that were incongruent were so because they used difference tests to support conclusions of equivalence. We found no applications of equivalence tests that were congruent with stated conclusions. The two equivalence tests that were applied were classified as incongruent because in one of these, the authors used an equivalence test to test for differences that were not found by an initial difference test and the other used an equivalence test to confirm an established difference. The majority of our sample (265/270) applied statistical tests of significance and reported the results of these statistical tests as the supporting evidence for their concluding statements. Of those that used statistical significance testing, 58.2% (149/256) stated conclusions that were logically incompatible with the results of the statistical tests that were used.</p>
</sec>
<sec id="s18" disp-level="subsect2">
<title>Overall (Hypothesis-Test-Conclusion) Congruence</title>
<p align="left">We defined Overall Congruence as a complete match between the hypothesis, statistical test, and conclusion. Therefore, the studies that could be evaluated for this variable were only those that presented all three items (<italic>n</italic> = 113). Studies that were coded as being congruent overall either had a difference hypothesis, used a difference test, and stated a conclusion relating to the state of difference or stated an equivalence hypothesis, used an equivalence test, and presented a conclusion relating to the observed state of equivalence or nonequivalence. There were 55 studies that met these criteria. All had difference hypotheses, used difference tests, and stated conclusions relating to an observed state of significant or nonsignificant difference. All studies (<italic>n</italic> = 25) that hypothesized equivalence used difference tests to test for equivalence. Although 22 of these studies concluded that the psychological treatments of interest were equivalent, the test statistics that were used in all of these cases were not appropriate to test for equivalence.</p>
</sec>
</sec>
</sec>
<sec id="s19">
<title>Discussion</title>
<sec id="s20" disp-level="subsect1">
<title>Inappropriately Applied Difference Tests and Testing Logic</title>
<p align="left"><xref ref-type="bibr" id="cr17-4" rid="c17">Kline (2004)</xref> and <xref ref-type="bibr" id="cr36-6" rid="c36">Wellek (2010)</xref> have both noted that significance tests seem to be selected and applied regardless of the research question or objective. We have found that within significance testing a similar preference seems to exist for difference tests. Difference tests seem to be selected and applied in treatment comparisons whether the objective is to examine these treatments for equivalence or for difference and also seem to be applied when the research is exploratory. Although this is not an incorrect application of these tests in cases where researchers want to use significance testing as a method of examining difference, it is inappropriate to form conclusions of equivalence from these tests. In line with the result we have obtained here, erroneous conclusions of equivalence from the application of significance tests for difference have been well noted in both the psychological and medical literature (<xref ref-type="bibr" id="cr6-2" rid="c6">Cribbie et al., 2004</xref>, <xref ref-type="bibr" id="cr4-2" rid="c4">2009</xref>; <xref ref-type="bibr" id="cr11-2" rid="c11">Greene et al., 2000</xref>; <xref ref-type="bibr" id="cr26-1" rid="c26">Rusticus &amp; Lovato, 2011</xref>; <xref ref-type="bibr" id="cr33-1" rid="c33">Tryon, 2001</xref>).</p>
<p align="left">There seems to be a clear need for a better understanding of what information can be gleaned from the application of a statistical procedure and for further guidelines for researchers who want to examine equivalence. This is evinced by finding 25 studies that specifically hypothesized that equivalence would be found, and 147 studies that concluded (based on statistical tests for difference) that the treatments of interest produced clinically equivalent results. All of these studies opted to use significance testing, and within significance testing, none selected equivalence tests. Instead, all of them used traditional difference tests, primarily Student&#8217;s <italic>t</italic> and ANOVA, to provide support for concluded states of equivalence. As noted earlier, this is logically incorrect, because difference test statistics cannot be used to support conclusions of equivalence. In contrast, 90 of the 91 studies that hypothesized a difference would be found or stated they were looking for a difference used difference statistics to evaluate their data. Over half of these (<italic>n =</italic> 55) stated conclusions that were appropriately supported by the test statistics that were used. Thirty-four, however, concluded equivalence&#8212;which again leads us to believe that although there is a need for processes to test for equivalence, there is some confusion about how to mathematically address this task if a researcher wants to evaluate between-groups equivalence within the framework of significance testing.</p>
<p align="left">It may simply be that researchers are unaware that this option is available. Most of us are familiar with significance tests in their more common forms (Student&#8217;s <italic>t</italic> test, ANOVA) because these tests are presented in nearly every introductory statistics course and every introductory statistics textbook. Some of us are also familiar with these tests because most, if not all, statistical software packages include them in a relatively easy-to-use format (e.g., R, SPSS, SAS, and even Microsoft Excel). Perhaps it is simply due to this ease of access and early exposure that difference-based significance testing remains the most highly prevalent data analysis testing method.</p>
<p align="left">For those wanting to read further about significance testing for equivalence, we briefly list the following resources. Please note, that the equivalence analogues to the difference tests listed here are subject to the same limitations and assumptions as their null hypothesis of no difference counterparts. One equivalence test that is analogous to Student&#8217;s <italic>t</italic> test is Schuirmann&#8217;s TOST. In this test, data are presumed to be normal and homoscedastic, and two hypotheses representing equivalence are generated and tested. Both of these hypotheses must be rejected in order to support that groups are equivalent (<xref ref-type="bibr" id="cr27-4" rid="c27">Schuirmann, 1987</xref>). When data are still normal, but have unequal variances, the analogue to the Welch <italic>t</italic> test, is the Schuirmann-Welch Test. The interested reader is referred to <xref ref-type="bibr" id="cr12-1" rid="c12">Gruman, Cribbie, and Arpin-Cribbie (2007)</xref> for a demonstration and study of this method. One test that has been proposed to be analogous to the ANOVA is the Wellek Test of Equivalence (<xref ref-type="bibr" id="cr18-1" rid="c18">Koh &amp; Cribbie, 2012</xref>; <xref ref-type="bibr" id="cr36-7" rid="c36">Wellek, 2010</xref>).</p>
</sec>
<sec id="s21" disp-level="subsect1">
<title>Lack of Hypotheses</title>
<p align="left">Another important observation was that of the full sample of 270 studies, 154 stated no hypotheses or specific purpose for the study other than to conduct comparisons. Although in many instances it is of importance to conduct nonpredictive exploratory comparisons between treatments, it seems unlikely that over half of the sample was engaging in exploratory research. If researchers want to use significance testing to examine their nonexploratory questions, it is integral to developing and following a statistical significance testing plan that a hypothesis is generated. If it is possible to establish a concrete study purpose, it follows that it is relatively simple to select a type of statistical test that is able to provide meaningful information about that hypothesis so that a logically appropriate conclusion (that is supported by the data) can be formulated.</p>
</sec>
<sec id="s22" disp-level="subsect1">
<title>Conclusion</title>
<p align="left">Our primary purpose for conducting this study was to describe how, in the behavioral science field, statistical tests are being applied in the comparison of two or more psychological treatments. As mentioned above, researchers use significance tests more than any other testing method. Researchers seem to apply tests of significance almost automatically without considering whether a significance test is the best test to help them answer their research questions. In the present sample, we have found that difference tests are the most prevalent significance testing choice. Difference tests seem to be similarly applied across the board whether the researcher is examining treatment outcome data for difference or for equivalence. Even in cases where the research is exploratory, difference tests are the most prevalent. Equivalence tests, in this domain, are underutilized. We have noted that many researchers inappropriately make equivalence-based conclusions when the result of a difference test is not statistically significant. This is inappropriate mathematical support for an equivalence conclusion because the test statistic that was selected did not test for equivalence.</p>
<p align="left">Although many statistical errors can be avoided within the domain of significance testing by selecting a test that is suited to examining the stated hypothesis and presenting only conclusions that are appropriate to the results of these tests, many statisticians and psychological researchers do not believe null hypothesis significance testing is useful.<xref ref-type="fn" id="fnc2-1" rid="fn2"><sup location="post">2</sup></xref> To remain neutral in this debate, we would like to clarify that we have focused our discussion on significance testing because, for better or worse, it is the most utilized means of data analysis in the behavioral sciences (<xref ref-type="bibr" id="cr3-2" rid="c3">Cohen, 1994</xref>). We recognize that significance testing, and specifically, equivalence testing, is not a one-size-fits-all statistical solution to describe the relationship between sets of data and groups of study participants. Data analysis methods should be selected on a case-by-case basis, very carefully, keeping in mind what can and cannot be inferred from these analyses. By better understanding the information a statistical test can provide and by selecting the most appropriate analytical method that we can to examine our research questions, the quality and accuracy of psychological treatment research can be improved.</p>
</sec>
</sec>
</body>
<back>
<fn-group content-type="footnotes">
<fn id="fn1"><label>1</label><p align="left">Further, depending on individual research practices, the Type I error rate may be higher than the nominal alpha (<xref ref-type="bibr" id="cr16-1" rid="c16">John, Loewenstein, &amp; Prelec, 2012</xref>; <xref ref-type="bibr" id="cr30-1" rid="c30">Simmons, Nelson, &amp; Simonsohn, 2011</xref>). For example, <xref ref-type="bibr" id="cr30-2" rid="c30">Simmons et al. (2011)</xref> point out that the number of false positives is much higher when researchers engage in questionable research practices, such as interim analysis and stopping the data collection upon achieving significance.</p></fn>
<fn id="fn2"><label>2</label><p align="left">Whereas proponents of significance testing point out that statistical significance tests are an easy to use, readily accessible means to objectively test hypotheses, it has long been questioned how well significance testing works to accurately examine our data due to the logic underlying the test and whether or not researchers have an accurate understanding of what information the test provides (<xref ref-type="bibr" id="cr7-1" rid="c7">Daniel, 1998</xref>; <xref ref-type="bibr" id="cr17-5" rid="c17">Kline, 2004</xref>). As the debate has been thoroughly documented, this is a brief overview, and we refer the interested reader to the cited resources for methodological demonstrations of alternatives to significance testing and a more complete picture of the debate.</p>
<p align="left">It has been pointed out that the <italic>p</italic> values that we come to know in our earliest introductory statistics courses in the form of the probability associated with values obtained by running a Student&#8217;s <italic>t</italic> test and in the analysis of variance (ANOVA), are potentially problematic because, alone, <italic>p</italic> values do not quantify the statistical evidence, nor do they provide details of the magnitude of the observed effect (<xref ref-type="bibr" id="cr3-3" rid="c3">Cohen, 1994</xref>; <xref ref-type="bibr" id="cr35-1" rid="c35">Wagenmakers, 2007</xref>). One method of solving this problem is to augment reports of significance test statistics with effect sizes and confidence intervals, and the APA recommends that researchers provide this additional information whenever possible (<xref ref-type="bibr" id="cr1-1" rid="c1">American Psychological Association, 2009</xref>; <xref ref-type="bibr" id="cr32-1" rid="c32">Thompson, 1999</xref>; <xref ref-type="bibr" id="cr39-1" rid="c39">Wilkinson &amp; APA Task Force on Statistical Inference, 1999</xref>).</p>
<p align="left">Confidence intervals have also been presented both as a stand-alone method of analysis and as a way to augment other statistical tests by illustrating the margin of error surrounding the observed data that are being mathematically tested (<xref ref-type="bibr" id="cr1-2" rid="c1">American Psychological Association, 2009</xref>; <xref ref-type="bibr" id="cr32-2" rid="c32">Thompson, 1999</xref>; <xref ref-type="bibr" id="cr39-2" rid="c39">Wilkinson &amp; APA Task Force on Statistical Inference, 1999</xref>). One advantage of using confidence intervals is that visually these can be easier to interpret because the interval contains both information about the magnitude of the observed effect and the measure of uncertainty associated with observed values (<xref ref-type="bibr" id="cr14-1" rid="c14">Hoekstra, Kiers, &amp; Johnson, 2010</xref>). Mau (1998) presents a discussion and formulas that can be used to examine the overlapping regions of confidence intervals as a means of assessing equivalence. The computation of a Bayes Factor has been presented as an alternative to significance testing (<italic>p</italic> values) (<xref ref-type="bibr" id="cr8-1" rid="c8">Garc&#237;a-P&#233;rez, 2012</xref>; <xref ref-type="bibr" id="cr35-2" rid="c35">Wagenmakers, 2007</xref>; <xref ref-type="bibr" id="cr38-1" rid="c38">Wetzels et al., 2011</xref>).</p>
</fn>
</fn-group>
<ref-list use-in-PI="yes"><title>References</title>
<ref><mixed-citation publication-type="book" meta="no" id="c1" xlink:type="simple"><person-group person-group-type="author"><collab xlink:type="simple">American Psychological Association</collab></person-group>. (<year>2009</year>). <source>Publication manual of the American Psychological Association</source> (<edition>6th ed.</edition>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c2" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Hauck</surname>, <given-names>W. W.</given-names></string-name></person-group> (<year>1983</year>). <article-title>A new procedure for testing equivalence in comparative bioavailability and other clinical trials</article-title>. <source>Communications in Statistics: Theory and Methods</source>, <volume>12</volume>, <fpage>2663</fpage>&#8211;<lpage>2692</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1080/03610928308828634</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c3" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1994</year>). <article-title>The earth is round (p &lt; .05)</article-title>. <source>American Psychologist</source>, <volume>49</volume>, <fpage>997</fpage>&#8211;<lpage>1003</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1037/0003-066X.49.12.997</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c4" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Cribbie</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Arpin-Cribbie</surname>, <given-names>C. A.</given-names></string-name>, &amp; <string-name><surname>Gruman</surname>, <given-names>J. A.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Tests of equivalence for one-way independent groups designs</article-title>. <source>Journal of Experimental Education</source>, <volume>78</volume>, <fpage>1</fpage>&#8211;<lpage>13</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1080/00220970903224552</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c6" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Cribbie</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Gruman</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Arpin-Cribbie</surname>, <given-names>C. A.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Recommendations for applying tests of equivalence</article-title>. <source>Journal of Clinical Psychology</source>, <volume>60</volume>, <fpage>1</fpage>&#8211;<lpage>10</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1002/jclp.10217</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c7" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Daniel</surname>, <given-names>L. G.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Statistical significance testing: A historical overview of misuse and misinterpretation with implications for the editorial policies of educational journals</article-title>. <source>Research in the Schools</source>, <volume>5</volume>, <fpage>23</fpage>&#8211;<lpage>32</lpage>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c8" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Garc&#237;a-P&#233;rez</surname>, <given-names>M. A.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Statistical conclusion validity: Some common threats and simple remedies</article-title>. <source>Frontiers in Psychology: Quantitative Psychology and Measurement</source>, <volume>3</volume>, <fpage>1</fpage>&#8211;<lpage>11</lpage>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c10" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Gordon</surname>, <given-names>R. S.</given-names></string-name></person-group> (<year>1985</year>). <article-title>Three current issues: The design and conduct of randomized clinical trials</article-title>. <source>IRB A Review of Human Subjects Research</source>, <volume>7</volume>, <page-range>1, 3, 12</page-range>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c11" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Greene</surname>, <given-names>W. L.</given-names></string-name>, <string-name><surname>Concato</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Feinstein</surname>, <given-names>A. R.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Claims of equivalence in medical research: Are they supported by the evidence?</article-title> <source>Annals of Internal Medicine</source>, <volume>132</volume>, <fpage>715</fpage>&#8211;<lpage>722</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.7326/0003-4819-132-9-200005020-00006</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c12" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Gruman</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Cribbie</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>Arpin-Cribbie</surname>, <given-names>C. A.</given-names></string-name></person-group> (<year>2007</year>). <article-title>The effects of heteroscedasticity on tests of equivalence</article-title>. <source>Journal of Modern Applied Statistical Methods</source>, <volume>6</volume>, <fpage>133</fpage>&#8211;<lpage>140</lpage>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="web-page" meta="no" id="c14" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Hoekstra</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Kiers</surname>, <given-names>H. A. L.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2010</year>). <article-title>The influence of presentation on the interpretation of inferential results (Invited Paper)</article-title>. <source>International Association of Statistical Education</source>. <comment>Available at</comment> <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" specific-use="live" xlink:href="http://130.203.133.150/viewdoc/summary?doi=10.1.1.205.795" xlink:type="simple">http://130.203.133.150/viewdoc/summary?doi=10.1.1.205.795</ext-link></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c15" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Howland</surname>, <given-names>R. H.</given-names></string-name></person-group> (<year>2009</year>). <article-title>What makes a generic medication generic?</article-title> <source>Journal of Psychosocial Nursing and Mental Health Services</source>, <volume>47</volume>, <fpage>17</fpage>&#8211;<lpage>20</lpage>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c16" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>John</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Loewenstein</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Prelec</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Measuring the prevalence of questionable research practices with incentives for truth telling</article-title>. <source>Psychological Science</source>, <volume>23</volume>, <fpage>524</fpage>&#8211;<lpage>532</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1177/0956797611430953</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="book" meta="no" id="c17" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Kline</surname>, <given-names>R. B.</given-names></string-name></person-group> (<year>2004</year>). <source>Beyond significance testing: Reforming data analysis methods in behavioral research</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1037/10693-000</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c18" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Koh</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Cribbie</surname>, <given-names>R. A.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Robust tests of equivalence for <italic>k</italic> independent groups</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>. <comment>Advance online publication</comment>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1111/j.2044-8317.2012.02056.x</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c21" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Mau</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1988</year>). <article-title>A statistical assessment of clinical equivalence</article-title>. <source>Statistics in Medicine</source>, <volume>7</volume>, <fpage>1267</fpage>&#8211;<lpage>1277</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1002/sim.4780071207</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c25" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Rogers</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Howard</surname>, <given-names>K. I.</given-names></string-name>, &amp; <string-name><surname>Vessey</surname>, <given-names>J. T.</given-names></string-name></person-group> (<year>1993</year>). <article-title>Using significance tests to evaluate equivalence between two experimental groups</article-title>. <source>Psychological Bulletin</source>, <volume>113</volume>, <fpage>553</fpage>&#8211;<lpage>565</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1037/0033-2909.113.3.553</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c26" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Rusticus</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Lovato</surname>, <given-names>C. Y.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Applying tests of equivalence for multiple group comparisons: Demonstration of the confidence interval approach</article-title>. <source>Practical Assessment, Research &amp; Evaluation</source>, <volume>16</volume>, <fpage>1</fpage>&#8211;<lpage>6</lpage>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c27" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Schuirmann</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>1987</year>). <article-title>A comparison of the two one-sided tests procedure and the power approach for assessing equivalence of average bioavailability</article-title>. <source>Journal of Pharmacokinetics and Biopharmaceutics</source>, <volume>15</volume>, <fpage>657</fpage>&#8211;<lpage>680</lpage>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c28" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Seaman</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Serlin</surname>, <given-names>R. C.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Equivalence confidence intervals for two-group comparisons of means</article-title>. <source>Psychological Methods</source>, <volume>3</volume>, <fpage>403</fpage>&#8211;<lpage>411</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1037/1082-989X.3.4.403</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c30" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Simmons</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>L. D.</given-names></string-name>, &amp; <string-name><surname>Simonsohn</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2011</year>). <article-title>False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant</article-title>. <source>Psychological Science</source>, <volume>22</volume>, <fpage>1359</fpage>&#8211;<lpage>1366</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1177/0956797611417632</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c31" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Stegner</surname>, <given-names>B. L.</given-names></string-name>, <string-name><surname>Bostrom</surname>, <given-names>A. G.</given-names></string-name>, &amp; <string-name><surname>Greenfield</surname>, <given-names>T. K.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Equivalence testing for use in psychosocial and services research: An introduction with examples</article-title>. <source>Evaluation and Program Planning</source>, <volume>19</volume>, <fpage>193</fpage>&#8211;<lpage>198</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1016/0149-7189(96)00011-0</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c32" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Thompson</surname>, <given-names>B.</given-names></string-name></person-group> (<year>1999</year>). <article-title>If statistical significance tests are broken/misused, what practices should replace them?</article-title> <source>Theory &amp; Psychology</source>, <volume>9</volume>. <fpage>165</fpage>&#8211;<lpage>181</lpage>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c33" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Tryon</surname>, <given-names>W. W.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Evaluating statistical difference, equivalence, and indeterminacy using inferential confidence intervals: An integrated alternative method of conducting null hypothesis statistical tests</article-title>. <source>Psychological Methods</source>, <volume>6</volume>, <fpage>371</fpage>&#8211;<lpage>386</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1037/1082-989X.6.4.371</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c34" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Tryon</surname>, <given-names>W. W.</given-names></string-name>, &amp; <string-name><surname>Lewis</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2008</year>). <article-title>An inferential confidence interval method of establishing statistical equivalence that corrects Tryon&#8217;s (2001). reduction factor</article-title>. <source>Psychological Methods</source>, <volume>13</volume>, <fpage>272</fpage>&#8211;<lpage>277</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1037/a0013158</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c35" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Wagenmakers</surname>, <given-names>E. J.</given-names></string-name></person-group> (<year>2007</year>). <article-title>A practical solution to the pervasive problems of p values</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>14</volume>, <fpage>779</fpage>&#8211;<lpage>804</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.3758/BF03194105</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="book" meta="no" id="c36" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Wellek</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2010</year>). <source>Testing statistical hypotheses of equivalence and noninferiority</source>, (<edition>2nd ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>CRC Press</publisher-name>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1201/EBK1439808184</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c37" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Westlake</surname>, <given-names>W. J.</given-names></string-name></person-group> (<year>1976</year>). <article-title>Symmetrical confidence intervals for bioequivalence trials</article-title>. <source>Biometrics</source>, <volume>32</volume>, <fpage>741</fpage>&#8211;<lpage>744</lpage>.</mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c38" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Wetzels</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Matzke</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Rouder</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Iverson</surname>, <given-names>G. J.</given-names></string-name>, &amp; <string-name><surname>Wagenmakers</surname>, <given-names>E. J.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Statistical evidence in experimental psychology: An empirical comparison using 855 t tests</article-title>. <source>Perspectives on Psychological Science</source>, <volume>6</volume>, <fpage>291</fpage>&#8211;<lpage>298</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1177/1745691611406923</pub-id></mixed-citation>
</ref>
<ref><mixed-citation publication-type="journal" meta="no" id="c39" xlink:type="simple"><person-group person-group-type="author"><string-name><surname>Wilkinson</surname>, <given-names>L.</given-names></string-name>, &amp; <collab xlink:type="simple">APA Task Force on Statistical Inference</collab></person-group>. (<year>1999</year>). <article-title>Statistical methods in psychology journals: Guidelines and explanations</article-title>. <source>American Psychologist</source>, <volume>54</volume>, <fpage>594</fpage>&#8211;<lpage>604</lpage>. doi:<pub-id pub-id-source="author-published" pub-id-type="doi">10.1037/0003-066X.54.8.594</pub-id></mixed-citation>
</ref>
</ref-list>
<app-group>
<app id="A" copyright="inherit"><title>Congruence Coding</title>
<sec id="s23">
<p align="left"><xref ref-type="table" id="tbc2-1" rid="tbl2">Table A1</xref> shows the distribution of statistical tests, hypotheses, and conclusions.<xref ref-type="table-anchor" rid="tbl2"/></p>
<p align="left"><xref ref-type="table" id="tbc3-1" rid="tbl3">Table A2</xref> shows the overall congruence coding.<xref ref-type="table-anchor" rid="tbl3"/></p>
</sec>
</app>
</app-group>
</back>
<floats-group>
<table-wrap id="tbl1" position="float" orientation="portrait" pagewide="no">
<label>1</label>
<caption><title>Distribution of Tests, Hypotheses, and Conclusions</title></caption>
<graphic copyright="inherit" id="tbl1a" xlink:href="cbs_45_4_320_tbl1a.tif" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" span="1"/>
<col align="char" char="." span="1"/>
</colgroup>
<thead>
<tr valign="bottom">
<th align="center" rowspan="1" colspan="1">Distributions</th>
<th align="center" rowspan="1" colspan="1"><italic>n</italic></th>
</tr>
</thead>
<tfoot valign="top">
<tr><td align="left" colspan="2" rowspan="1"><sup location="post">a</sup> Two studies used both a difference test and an equivalence test to evaluate their primary hypothesis.&#8195;<sup location="post">b</sup>&#8201;The five studies that did not use or did not indicate the use of statistical testing were not included in this analysis.</td></tr></tfoot>
<tbody>
<tr valign="top">
<td rowspan="1" colspan="1">Statistical tests (<italic>n</italic> = 270<sup location="post">a</sup>)</td>
<td rowspan="1" colspan="1"/>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Used difference tests to evaluate treatment comparison</td>
<td rowspan="1" colspan="1">265</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Used equivalence tests to evaluate treatment comparison</td>
<td rowspan="1" colspan="1">2</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Did not use (or did not state the use of) statistics</td>
<td rowspan="1" colspan="1">5</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Hypotheses (<italic>n</italic> = 270)</td>
<td rowspan="1" colspan="1"/>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Primary hypothesis related to state of difference</td>
<td rowspan="1" colspan="1">91</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Primary hypothesis related to a state of equivalence</td>
<td rowspan="1" colspan="1">25</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Hypothesis was stated as &#8220;vs.&#8221; or &#8220;comparison&#8221;</td>
<td rowspan="1" colspan="1">154</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Conclusions (<italic>n</italic> = 265<sup location="post">b</sup>)</td>
<td rowspan="1" colspan="1"/>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Conclusion relating to state of difference</td>
<td rowspan="1" colspan="1">109</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Conclusion relating to state of equivalence</td>
<td rowspan="1" colspan="1">147</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Conclusion listed as a set of descriptives</td>
<td rowspan="1" colspan="1">7</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">&#8195;Conclusion stated as &#8220;inconclusive&#8221;</td>
<td rowspan="1" colspan="1">2</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="tbl2" position="anchor" orientation="portrait" pagewide="no">
<label>A1</label>
<caption><title>Overall Congruence Coding Conceptualization</title></caption>
<graphic copyright="inherit" id="tbl2a" xlink:href="cbs_45_4_320_tbl2a.tif" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" span="1"/>
<col align="left" span="1"/>
<col align="left" span="1"/>
<col align="center" span="1"/>
</colgroup>
<thead>
<tr valign="bottom">
<th align="center" rowspan="1" colspan="1">Hypothesis</th>
<th align="center" rowspan="1" colspan="1">Test</th>
<th align="center" rowspan="1" colspan="1">Conclusion</th>
<th align="center" rowspan="1" colspan="1">Congruence</th>
</tr>
</thead>
<tbody>
<tr valign="top">
<td rowspan="1" colspan="1">Stated a hypothesis</td>
<td rowspan="1" colspan="1">Used a test appropriate to test the hypothesis</td>
<td rowspan="1" colspan="1">Stated a conclusion logically corresponding to the test that was conducted</td>
<td rowspan="1" colspan="1">Yes</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Stated a hypothesis</td>
<td rowspan="1" colspan="1">Used a test inappropriate to test the hypothesis</td>
<td rowspan="1" colspan="1">Stated a conclusion logically corresponding to the test that was conducted</td>
<td rowspan="1" colspan="1">No</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Stated a hypothesis</td>
<td rowspan="1" colspan="1">Used a test appropriate to the hypothesis</td>
<td rowspan="1" colspan="1">Stated a conclusion not logically corresponding to the test that was conducted</td>
<td rowspan="1" colspan="1">No</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="tbl3" position="anchor" orientation="portrait" pagewide="no">
<label>A2</label>
<caption><title>Overall Congruence Coding in Greater Detail</title></caption>
<graphic copyright="inherit" id="tbl3a" xlink:href="cbs_45_4_320_tbl3a.tif" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" span="1"/>
<col align="left" span="1"/>
<col align="left" span="1"/>
<col align="center" span="1"/>
</colgroup>
<thead>
<tr valign="bottom">
<th align="center" rowspan="1" colspan="1">Hypothesis</th>
<th align="center" rowspan="1" colspan="1">Test type</th>
<th align="center" rowspan="1" colspan="1">Conclusion statement</th>
<th align="center" rowspan="1" colspan="1">Congruent</th>
</tr>
</thead>
<tbody>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted difference</td>
<td rowspan="1" colspan="1">Difference</td>
<td rowspan="1" colspan="1">Equivalent</td>
<td rowspan="1" colspan="1">No</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted no difference</td>
<td rowspan="1" colspan="1">Difference</td>
<td rowspan="1" colspan="1">Equivalent</td>
<td rowspan="1" colspan="1">No</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted difference</td>
<td rowspan="1" colspan="1">Difference</td>
<td rowspan="1" colspan="1">Not equivalent</td>
<td rowspan="1" colspan="1">No</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted no difference</td>
<td rowspan="1" colspan="1">Difference</td>
<td rowspan="1" colspan="1">Not equivalent</td>
<td rowspan="1" colspan="1">No</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted difference</td>
<td rowspan="1" colspan="1">Difference</td>
<td rowspan="1" colspan="1">Found no significant difference</td>
<td rowspan="1" colspan="1">Yes</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted no difference</td>
<td rowspan="1" colspan="1">Difference</td>
<td rowspan="1" colspan="1">Found no significant difference</td>
<td rowspan="1" colspan="1">Yes</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted difference</td>
<td rowspan="1" colspan="1">Difference</td>
<td rowspan="1" colspan="1">Found significant difference</td>
<td rowspan="1" colspan="1">Yes</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted no difference</td>
<td rowspan="1" colspan="1">Difference</td>
<td rowspan="1" colspan="1">Found significant difference</td>
<td rowspan="1" colspan="1">Yes</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted equivalent</td>
<td rowspan="1" colspan="1">Equivalence</td>
<td rowspan="1" colspan="1">Equivalent</td>
<td rowspan="1" colspan="1">Yes</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted not equivalent</td>
<td rowspan="1" colspan="1">Equivalence</td>
<td rowspan="1" colspan="1">Equivalent</td>
<td rowspan="1" colspan="1">Yes</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted equivalent</td>
<td rowspan="1" colspan="1">Equivalence</td>
<td rowspan="1" colspan="1">Not equivalent</td>
<td rowspan="1" colspan="1">Yes</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted not equivalent</td>
<td rowspan="1" colspan="1">Equivalence</td>
<td rowspan="1" colspan="1">Not equivalent</td>
<td rowspan="1" colspan="1">Yes</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted equivalent</td>
<td rowspan="1" colspan="1">Equivalence</td>
<td rowspan="1" colspan="1">Found no significant difference</td>
<td rowspan="1" colspan="1">No</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted not equivalent</td>
<td rowspan="1" colspan="1">Equivalence</td>
<td rowspan="1" colspan="1">Found no significant difference</td>
<td rowspan="1" colspan="1">No</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted equivalent</td>
<td rowspan="1" colspan="1">Equivalence</td>
<td rowspan="1" colspan="1">Found significant difference</td>
<td rowspan="1" colspan="1">No</td>
</tr>
<tr valign="top">
<td rowspan="1" colspan="1">Predicted not equivalent</td>
<td rowspan="1" colspan="1">Equivalence</td>
<td rowspan="1" colspan="1">Found significant difference</td>
<td rowspan="1" colspan="1">No</td>
</tr>
</tbody>
</table>
</table-wrap>
<fig id="fig1">
<label>1</label>
<caption>
<p align="left">Summary of study conclusion statements of equivalence from difference tests (<italic>n</italic> = 147).</p>
</caption>
<graphic copyright="inherit" id="fig1a" xlink:href="cbs_45_4_320_fig1a.tif" xlink:type="simple"/>
</fig>
</floats-group>
</article>